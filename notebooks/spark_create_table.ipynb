{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:54:48.643173Z",
     "start_time": "2025-05-07T09:54:48.633622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "id": "bbd5212737d98758",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:54:48.798316Z",
     "start_time": "2025-05-07T09:54:48.742616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SimpleSparkSession:\n",
    "    \"\"\"Simple Spark session builder for Jupyter notebooks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            app_name=\"Jupyter Spark Session\",\n",
    "            master=\"local[*]\",\n",
    "            spark_config=None,\n",
    "            enable_hive_support=False,\n",
    "            # S3 configuration\n",
    "            s3_bucket_name=None,\n",
    "            s3_endpoint=None,\n",
    "            s3_access_key=None,\n",
    "            s3_secret_key=None,\n",
    "            s3_region=\"us-east-1\",\n",
    "            s3_path_style_access=True,\n",
    "            # PostgreSQL configuration\n",
    "            postgres_config=None,\n",
    "            # Package configuration\n",
    "            packages=None\n",
    "    ):\n",
    "        self.app_name = app_name\n",
    "        self.master = master\n",
    "        self.spark_config = spark_config or {}\n",
    "        self.enable_hive_support = enable_hive_support\n",
    "\n",
    "        # S3 config\n",
    "        self.s3_bucket_name = s3_bucket_name\n",
    "        self.s3_endpoint = s3_endpoint\n",
    "        self.s3_access_key = s3_access_key\n",
    "        self.s3_secret_key = s3_secret_key\n",
    "        self.s3_region = s3_region\n",
    "        self.s3_path_style_access = s3_path_style_access\n",
    "\n",
    "        # PostgreSQL config\n",
    "        self.postgres_config = postgres_config\n",
    "        self.jdbc_driver_path: Optional[str] = None\n",
    "\n",
    "        # Packages\n",
    "        self.packages = packages or []\n",
    "\n",
    "        self._session = None\n",
    "\n",
    "    def build_session(self):\n",
    "        \"\"\"Build and return a SparkSession\"\"\"\n",
    "        if self._session is not None:\n",
    "            return self._session\n",
    "\n",
    "        # Start building the session\n",
    "        builder = SparkSession.builder.appName(self.app_name).master(self.master)\n",
    "\n",
    "        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    " \\\n",
    "            # Add Hive support if requested\n",
    "        if self.enable_hive_support:\n",
    "            builder = builder.enableHiveSupport()\n",
    "\n",
    "        if self.jdbc_driver_path:\n",
    "            builder = builder.config(\"spark.driver.extraClassPath\", self.jdbc_driver_path)\n",
    "            builder = builder.config(\"spark.executor.extraClassPath\", self.jdbc_driver_path)\n",
    "\n",
    "        # Add all configuration options\n",
    "        for key, value in self.spark_config.items():\n",
    "            builder = builder.config(key, value)\n",
    "\n",
    "        # Configure packages\n",
    "        if self.packages:\n",
    "            packages = \",\".join(self.packages)\n",
    "            builder = builder.config(\"spark.jars.packages\", packages)\n",
    "\n",
    "        # Add S3 configuration if credentials provided\n",
    "        if self.s3_access_key and self.s3_secret_key:\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.access.key\", self.s3_access_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.secret.key\", self.s3_secret_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                                     \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "            # Config for non-AWS S3\n",
    "            if self.s3_endpoint:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint\", self.s3_endpoint)\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint.region\", self.s3_region)\n",
    "\n",
    "            # Path style access for non-AWS implementations\n",
    "            if self.s3_path_style_access:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\")\n",
    "\n",
    "        # Build the session\n",
    "        logger.info(f\"Building Spark session with app name: {self.app_name}, master: {self.master}\")\n",
    "        self._session = builder.getOrCreate()\n",
    "\n",
    "        return self._session\n",
    "\n",
    "    def get_session(self):\n",
    "        \"\"\"Get the current SparkSession or create a new one\"\"\"\n",
    "        return self.build_session()\n",
    "\n",
    "    def stop_session(self):\n",
    "        \"\"\"Stop the current Spark session if it exists\"\"\"\n",
    "        if self._session is not None:\n",
    "            self._session.stop()\n",
    "            self._session = None\n",
    "            logger.info(\"Spark session stopped\")"
   ],
   "id": "767abb7a7b2e068c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:46:57.271552Z",
     "start_time": "2025-05-07T10:46:57.252376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SimpleSparkSession(\n",
    "    app_name=\"Create table notebook\",\n",
    "    packages=[\n",
    "        \"org.postgresql:postgresql:42.5.4\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.426\"\n",
    "    ],\n",
    "    s3_access_key=os.getenv(\"S3_ACCESS_KEY\"),\n",
    "    s3_secret_key=os.getenv(\"S3_SECRET_KEY\"),\n",
    "    s3_endpoint=os.getenv(\"S3_ENDPOINT\"),\n",
    "    s3_region=\"garage\",\n",
    "    s3_path_style_access=True,\n",
    "    postgres_config={\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"currentSchema\": \"public\"\n",
    "    },\n",
    "    enable_hive_support=False,\n",
    "    s3_bucket_name=\"traffy-troffi\"\n",
    ").get_session()"
   ],
   "id": "486ef07731951abd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Building Spark session with app name: Create table notebook, master: local[*]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:50:24.334018Z",
     "start_time": "2025-05-07T10:50:24.332300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "create_table_query = \"\"\"\n",
    "                     CREATE TABLE IF NOT EXISTS public.traffy_fondue\n",
    "                     (\n",
    "                         ticket_id      VARCHAR(20)      NOT NULL PRIMARY KEY,\n",
    "                         complaint      TEXT             NOT NULL,\n",
    "                         timestamp      TIMESTAMP        NOT NULL,\n",
    "                         image          TEXT             NOT NULL,\n",
    "                         image_after    TEXT             NOT NULL,\n",
    "                         latitude       DOUBLE PRECISION NOT NULL,\n",
    "                         longitude      DOUBLE PRECISION NOT NULL,\n",
    "                         district       VARCHAR(128)     NOT NULL,\n",
    "                         subdistrict    VARCHAR(128)     NOT NULL,\n",
    "                         categories     VARCHAR(128)[]   NOT NULL,\n",
    "                         categories_idx REAL[]           NOT NULL\n",
    "                     );\n",
    "\n",
    "                     ALTER TABLE public.traffy_fondue\n",
    "                         OWNER TO postgres;\n",
    "\n",
    "                     CREATE UNIQUE INDEX IF NOT EXISTS idx_traffy_fondue_ticket_id ON public.traffy_fondue (ticket_id);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_district ON public.traffy_fondue (district);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_subdistrict ON public.traffy_fondue (subdistrict);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_district_subdistrict ON public.traffy_fondue (district, subdistrict);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_timestamp ON public.traffy_fondue (timestamp);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_categories ON public.traffy_fondue USING GIN (categories);\n",
    "\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_time_district ON public.traffy_fondue (timestamp, district);\n",
    "                     CREATE INDEX IF NOT EXISTS idx_traffy_fondue_time_district_subdistrict ON public.traffy_fondue (timestamp, district, subdistrict);\n",
    "\n",
    "                     ALTER TABLE public.traffy_fondue\n",
    "                         ALTER COLUMN district SET STATISTICS 1000;\n",
    "                     ALTER TABLE public.traffy_fondue\n",
    "                         ALTER COLUMN subdistrict SET STATISTICS 1000;\n",
    "                     \"\"\""
   ],
   "id": "55c0ca33863d606e",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:56:14.585128Z",
     "start_time": "2025-05-07T10:56:14.581260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_postgres_table():\n",
    "    # Define connection parameters\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5432/traffy-troffi\"\n",
    "    properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"troffi\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    connection = spark._jvm.java.sql.DriverManager.getConnection(\n",
    "        jdbc_url, properties[\"user\"], properties[\"password\"])\n",
    "\n",
    "    # Create a statement\n",
    "    statement = connection.createStatement()\n",
    "\n",
    "    # Split the query by semicolons but preserve semicolons in other contexts\n",
    "    import re\n",
    "    sql_statements = re.split(r';(?=(?:[^\\']*\\'[^\\']*\\')*[^\\']*$)', create_table_query)\n",
    "\n",
    "    # Execute each statement\n",
    "    for sql in sql_statements:\n",
    "        sql = sql.strip()\n",
    "        if sql:  # Skip empty statements\n",
    "            print(f\"Executing: {sql}\")\n",
    "            statement.execute(sql)\n",
    "\n",
    "    print(\"All DDL statements executed successfully\")"
   ],
   "id": "c3e05e698debed69",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:56:15.608533Z",
     "start_time": "2025-05-07T10:56:15.569515Z"
    }
   },
   "cell_type": "code",
   "source": "create_postgres_table()",
   "id": "b660449316541fd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: CREATE TABLE IF NOT EXISTS public.traffy_fondue\n",
      "                     (\n",
      "                         ticket_id      VARCHAR(20)      NOT NULL PRIMARY KEY,\n",
      "                         complaint      TEXT             NOT NULL,\n",
      "                         timestamp      TIMESTAMP        NOT NULL,\n",
      "                         image          TEXT             NOT NULL,\n",
      "                         image_after    TEXT             NOT NULL,\n",
      "                         latitude       DOUBLE PRECISION NOT NULL,\n",
      "                         longitude      DOUBLE PRECISION NOT NULL,\n",
      "                         district       VARCHAR(128)     NOT NULL,\n",
      "                         subdistrict    VARCHAR(128)     NOT NULL,\n",
      "                         categories     VARCHAR(128)[]   NOT NULL,\n",
      "                         categories_idx REAL[]           NOT NULL\n",
      "                     )\n",
      "Executing: ALTER TABLE public.traffy_fondue\n",
      "                         OWNER TO postgres\n",
      "Executing: CREATE UNIQUE INDEX IF NOT EXISTS idx_traffy_fondue_ticket_id ON public.traffy_fondue (ticket_id)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_district ON public.traffy_fondue (district)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_subdistrict ON public.traffy_fondue (subdistrict)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_district_subdistrict ON public.traffy_fondue (district, subdistrict)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_timestamp ON public.traffy_fondue (timestamp)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_categories ON public.traffy_fondue USING GIN (categories)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_time_district ON public.traffy_fondue (timestamp, district)\n",
      "Executing: CREATE INDEX IF NOT EXISTS idx_traffy_fondue_time_district_subdistrict ON public.traffy_fondue (timestamp, district, subdistrict)\n",
      "Executing: ALTER TABLE public.traffy_fondue\n",
      "                         ALTER COLUMN district SET STATISTICS 1000\n",
      "Executing: ALTER TABLE public.traffy_fondue\n",
      "                         ALTER COLUMN subdistrict SET STATISTICS 1000\n",
      "All DDL statements executed successfully\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac940868883b88c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
