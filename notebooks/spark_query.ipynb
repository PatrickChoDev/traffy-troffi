{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:44.956701Z",
     "start_time": "2025-05-07T14:31:44.950999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "id": "4a7bdb5dcf241719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:45.225372Z",
     "start_time": "2025-05-07T14:31:45.217342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SimpleSparkSession:\n",
    "    \"\"\"Simple Spark session builder for Jupyter notebooks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            app_name=\"Jupyter Spark Session\",\n",
    "            master=\"local[*]\",\n",
    "            spark_config=None,\n",
    "            enable_hive_support=False,\n",
    "            # S3 configuration\n",
    "            s3_bucket_name=None,\n",
    "            s3_endpoint=None,\n",
    "            s3_access_key=None,\n",
    "            s3_secret_key=None,\n",
    "            s3_region=\"us-east-1\",\n",
    "            s3_path_style_access=True,\n",
    "            # PostgreSQL configuration\n",
    "            postgres_config=None,\n",
    "            # Package configuration\n",
    "            packages=None\n",
    "    ):\n",
    "        self.app_name = app_name\n",
    "        self.master = master\n",
    "        self.spark_config = spark_config or {}\n",
    "        self.enable_hive_support = enable_hive_support\n",
    "\n",
    "        # S3 config\n",
    "        self.s3_bucket_name = s3_bucket_name\n",
    "        self.s3_endpoint = s3_endpoint\n",
    "        self.s3_access_key = s3_access_key\n",
    "        self.s3_secret_key = s3_secret_key\n",
    "        self.s3_region = s3_region\n",
    "        self.s3_path_style_access = s3_path_style_access\n",
    "\n",
    "        # PostgreSQL config\n",
    "        self.postgres_config = postgres_config\n",
    "        self.jdbc_driver_path: Optional[str] = None\n",
    "\n",
    "        # Packages\n",
    "        self.packages = packages or []\n",
    "\n",
    "        self._session = None\n",
    "\n",
    "    def build_session(self):\n",
    "        \"\"\"Build and return a SparkSession\"\"\"\n",
    "        if self._session is not None:\n",
    "            return self._session\n",
    "\n",
    "        # Start building the session\n",
    "        builder = SparkSession.builder.appName(self.app_name).master(self.master)\n",
    "\n",
    "        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    " \\\n",
    "            # Add Hive support if requested\n",
    "        if self.enable_hive_support:\n",
    "            builder = builder.enableHiveSupport()\n",
    "\n",
    "        if self.jdbc_driver_path:\n",
    "            builder = builder.config(\"spark.driver.extraClassPath\", self.jdbc_driver_path)\n",
    "            builder = builder.config(\"spark.executor.extraClassPath\", self.jdbc_driver_path)\n",
    "\n",
    "        # Add all configuration options\n",
    "        for key, value in self.spark_config.items():\n",
    "            builder = builder.config(key, value)\n",
    "\n",
    "        # Configure packages\n",
    "        if self.packages:\n",
    "            packages = \",\".join(self.packages)\n",
    "            builder = builder.config(\"spark.jars.packages\", packages)\n",
    "\n",
    "        # Add S3 configuration if credentials provided\n",
    "        if self.s3_access_key and self.s3_secret_key:\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.access.key\", self.s3_access_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.secret.key\", self.s3_secret_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                                     \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "            # Config for non-AWS S3\n",
    "            if self.s3_endpoint:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint\", self.s3_endpoint)\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint.region\", self.s3_region)\n",
    "\n",
    "            # Path style access for non-AWS implementations\n",
    "            if self.s3_path_style_access:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\")\n",
    "\n",
    "        # Build the session\n",
    "        logger.info(f\"Building Spark session with app name: {self.app_name}, master: {self.master}\")\n",
    "        self._session = builder.getOrCreate()\n",
    "\n",
    "        return self._session\n",
    "\n",
    "    def get_session(self):\n",
    "        \"\"\"Get the current SparkSession or create a new one\"\"\"\n",
    "        return self.build_session()\n",
    "\n",
    "    def stop_session(self):\n",
    "        \"\"\"Stop the current Spark session if it exists\"\"\"\n",
    "        if self._session is not None:\n",
    "            self._session.stop()\n",
    "            self._session = None\n",
    "            logger.info(\"Spark session stopped\")"
   ],
   "id": "8b2bb162eb09d731",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:50.910680Z",
     "start_time": "2025-05-07T14:31:48.668937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SimpleSparkSession(\n",
    "    app_name=\"Data Analysis Notebook\",\n",
    "    packages=[\n",
    "        \"org.postgresql:postgresql:42.5.4\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.426\"\n",
    "    ],\n",
    "    s3_access_key=os.getenv(\"S3_ACCESS_KEY\"),\n",
    "    s3_secret_key=os.getenv(\"S3_SECRET_KEY\"),\n",
    "    s3_endpoint=os.getenv(\"S3_ENDPOINT\"),\n",
    "    s3_region=\"garage\",\n",
    "    s3_path_style_access=True,\n",
    "    postgres_config={\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"currentSchema\": \"public\"\n",
    "    },\n",
    "    enable_hive_support=False,\n",
    "    s3_bucket_name=\"traffy-troffi\"\n",
    ").get_session()"
   ],
   "id": "34bc065afbc5a0fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Building Spark session with app name: Data Analysis Notebook, master: local[*]\n",
      "25/05/07 21:31:49 WARN Utils: Your hostname, PatrickChoDevMacbook.local resolves to a loopback address: 127.0.0.1; using 192.168.158.3 instead (on interface en0)\n",
      "25/05/07 21:31:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/patrick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/patrick/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8b89cfef-70a6-45cc-848e-97ca380ef740;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.5.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.426 in central\n",
      ":: resolution report :: resolve 84ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.426 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.426] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   1   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8b89cfef-70a6-45cc-848e-97ca380ef740\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/patrick/Desktop/Workspace/Projects/traffy-troffi/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 21:31:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/07 21:31:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/07 21:31:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:54.313149Z",
     "start_time": "2025-05-07T14:31:53.674801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).printSchema()"
   ],
   "id": "3831892c8309fa70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticket_id: string (nullable = true)\n",
      " |-- complaint: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- image: string (nullable = true)\n",
      " |-- image_after: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- subdistrict: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories_idx: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:04.742315Z",
     "start_time": "2025-05-07T14:32:04.736952Z"
    }
   },
   "cell_type": "code",
   "source": "import pyspark.sql.functions as F",
   "id": "4363751b6abf68e1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:12.155924Z",
     "start_time": "2025-05-07T14:32:10.417772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).filter(\n",
    "    (F.col(\"district\") == 'ราชเทวี') & (F.array_contains('categories', 'ถนน'))).show()"
   ],
   "id": "d6a376a72faa63c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+-----------------+\n",
      "|  ticket_id|           complaint|           timestamp|               image|         image_after|latitude|longitude|district|subdistrict|          categories|   categories_idx|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+-----------------+\n",
      "|2024-UPZB93|รบกวนช่วยทำพื้นถน...|2024-09-09 11:44:...|https://storage.g...|https://storage.g...| 13.7548|100.54201| ราชเทวี|   ถนนพญาไท|               [ถนน]|            [9.0]|\n",
      "|2024-KTGPQY|บริเวณนี้มีการเปิ...|2024-05-07 14:25:...|https://storage.g...|https://storage.g...|13.75871|100.54239| ราชเทวี|   ถนนพญาไท|               [ถนน]|            [9.0]|\n",
      "|2024-K3PRE6|จุดนี้มืดและเปลี่...|2024-11-01 05:02:...|https://storage.g...|https://storage.g...|13.75672|100.52418| ราชเทวี|  ทุ่งพญาไท|[แสงสว่าง, ถนน, ค...| [23.0, 9.0, 6.0]|\n",
      "|2024-BRUQ8W|ถนนราชวิถีช่วงก่อ...|2024-05-02 09:46:...|https://storage.g...|https://storage.g...|13.76619| 100.5359| ราชเทวี|  ทุ่งพญาไท|        [ถนน, จราจร]|       [9.0, 7.0]|\n",
      "|2024-628FCL|*ถนน พระราม6 มุ่ง...|2024-07-24 04:24:...|https://storage.g...|https://storage.g...|13.76074|100.52516| ราชเทวี|  ทุ่งพญาไท|               [ถนน]|            [9.0]|\n",
      "|2024-9HFGT9|ถนนพระราม6 มีซากร...|2024-09-13 01:50:...|https://storage.g...|https://storage.g...|13.75678|100.52396| ราชเทวี|  ทุ่งพญาไท|      [กีดขวาง, ถนน]|       [2.0, 9.0]|\n",
      "|2024-K3NRY2| ร้านค้าปิดถนนขายของ|2024-05-08 11:30:...|https://storage.g...|https://storage.g...|13.75189|100.53489| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-M3ANTN|            ถนนชำรุด|2024-10-02 06:52:...|https://storage.g...|https://storage.g...| 13.7608|100.52797| ราชเทวี|  ทุ่งพญาไท|               [ถนน]|            [9.0]|\n",
      "|2024-FC8ERW|พบว่ามีบุคคลนำพื้...|2024-05-02 09:43:...|https://storage.g...|https://storage.g...|13.76566|100.52842| ราชเทวี|  ทุ่งพญาไท|      [กีดขวาง, ถนน]|       [2.0, 9.0]|\n",
      "|2024-BNVWBY|ซอยเพชรบุรี 5 แขว...|2024-05-14 11:41:...|https://storage.g...|https://storage.g...|13.75639| 100.5293| ราชเทวี|  ทุ่งพญาไท|[น้ำท่วม, ถนน, ร้...|[12.0, 9.0, 15.0]|\n",
      "|2024-HCBFHK|มีร้านขายอาหาร แล...|2024-05-13 06:27:...|https://storage.g...|https://storage.g...|13.75323|100.54204| ราชเทวี|   มักกะสัน|      [กีดขวาง, ถนน]|       [2.0, 9.0]|\n",
      "|2024-KNDDBH|เกาะหลางถนนไม่สวยเลย|2024-09-04 03:25:...|https://storage.g...|https://storage.g...|13.74964|100.54085| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-QYCGRU|หน้าปากซอยทางเข้า...|2024-05-27 07:55:...|https://storage.g...|https://storage.g...|13.75192|100.52574| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-GPU2K3|ตรงใต้สะพานห้างพา...|2024-10-17 04:05:...|https://storage.g...|https://storage.g...|13.74987|100.54216| ราชเทวี|   มักกะสัน|[สะพาน, ถนน, ความ...| [17.0, 9.0, 5.0]|\n",
      "|2024-N2TCL8|ถนนชำรุดเป็นหลุมเ...|2024-05-06 05:34:...|https://storage.g...|https://storage.g...|13.74955|100.53108| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-DPYNL7|หลุมบ่อมีน้ำขังสะ...|2024-05-27 07:59:...|https://storage.g...|https://storage.g...|13.75182|100.52591| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-6XHBZ9|หน้าห้างแพลตตินั่...|2024-05-09 07:47:...|https://storage.g...|https://storage.g...|13.75056|100.53909| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-DPZYG9|หลุมบ่อกลางถนนบริ...|2024-07-09 05:53:...|https://storage.g...|https://storage.g...|13.75215|100.52589| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|            [9.0]|\n",
      "|2024-3HXNUZ|กิ่งไม้ยาวมาออกมา...|2024-05-08 08:00:...|https://storage.g...|https://storage.g...|13.76115|100.53685| ราชเทวี|   ถนนพญาไท|               [ถนน]|            [9.0]|\n",
      "|2024-BFAX79|หน้าห้างแพลตตินั่...|2024-05-09 07:40:...|https://storage.g...|https://storage.g...|13.75056|100.53899| ราชเทวี|ถนนเพชรบุรี|      [กีดขวาง, ถนน]|       [2.0, 9.0]|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:17.965653Z",
     "start_time": "2025-05-07T14:32:17.684488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).filter(\n",
    "    (F.col(\"district\") == 'ราชเทวี') & (F.array_contains('categories', 'ถนน'))).count()"
   ],
   "id": "f06acc14a916a03f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54e934b33b9f6a66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
