{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7bdb5dcf241719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:44.956701Z",
     "start_time": "2025-05-07T14:31:44.950999Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2bb162eb09d731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:45.225372Z",
     "start_time": "2025-05-07T14:31:45.217342Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SimpleSparkSession:\n",
    "    \"\"\"Simple Spark session builder for Jupyter notebooks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            app_name=\"Jupyter Spark Session\",\n",
    "            master=\"local[*]\",\n",
    "            spark_config=None,\n",
    "            enable_hive_support=False,\n",
    "            # S3 configuration\n",
    "            s3_bucket_name=None,\n",
    "            s3_endpoint=None,\n",
    "            s3_access_key=None,\n",
    "            s3_secret_key=None,\n",
    "            s3_region=\"us-east-1\",\n",
    "            s3_path_style_access=True,\n",
    "            # PostgreSQL configuration\n",
    "            postgres_config=None,\n",
    "            # Package configuration\n",
    "            packages=None\n",
    "    ):\n",
    "        self.app_name = app_name\n",
    "        self.master = master\n",
    "        self.spark_config = spark_config or {}\n",
    "        self.enable_hive_support = enable_hive_support\n",
    "\n",
    "        # S3 config\n",
    "        self.s3_bucket_name = s3_bucket_name\n",
    "        self.s3_endpoint = s3_endpoint\n",
    "        self.s3_access_key = s3_access_key\n",
    "        self.s3_secret_key = s3_secret_key\n",
    "        self.s3_region = s3_region\n",
    "        self.s3_path_style_access = s3_path_style_access\n",
    "\n",
    "        # PostgreSQL config\n",
    "        self.postgres_config = postgres_config\n",
    "        self.jdbc_driver_path: Optional[str] = None\n",
    "\n",
    "        # Packages\n",
    "        self.packages = packages or []\n",
    "\n",
    "        self._session = None\n",
    "\n",
    "    def build_session(self):\n",
    "        \"\"\"Build and return a SparkSession\"\"\"\n",
    "        if self._session is not None:\n",
    "            return self._session\n",
    "\n",
    "        # Start building the session\n",
    "        builder = SparkSession.builder.appName(self.app_name).master(self.master)\n",
    "\n",
    "        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    " \\\n",
    "            # Add Hive support if requested\n",
    "        if self.enable_hive_support:\n",
    "            builder = builder.enableHiveSupport()\n",
    "\n",
    "        if self.jdbc_driver_path:\n",
    "            builder = builder.config(\"spark.driver.extraClassPath\", self.jdbc_driver_path)\n",
    "            builder = builder.config(\"spark.executor.extraClassPath\", self.jdbc_driver_path)\n",
    "\n",
    "        # Add all configuration options\n",
    "        for key, value in self.spark_config.items():\n",
    "            builder = builder.config(key, value)\n",
    "\n",
    "        # Configure packages\n",
    "        if self.packages:\n",
    "            packages = \",\".join(self.packages)\n",
    "            builder = builder.config(\"spark.jars.packages\", packages)\n",
    "\n",
    "        # Add S3 configuration if credentials provided\n",
    "        if self.s3_access_key and self.s3_secret_key:\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.access.key\", self.s3_access_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.secret.key\", self.s3_secret_key)\n",
    "            builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                                     \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "            # Config for non-AWS S3\n",
    "            if self.s3_endpoint:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint\", self.s3_endpoint)\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.endpoint.region\", self.s3_region)\n",
    "\n",
    "            # Path style access for non-AWS implementations\n",
    "            if self.s3_path_style_access:\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                builder = builder.config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\")\n",
    "\n",
    "        # Build the session\n",
    "        logger.info(f\"Building Spark session with app name: {self.app_name}, master: {self.master}\")\n",
    "        self._session = builder.getOrCreate()\n",
    "\n",
    "        return self._session\n",
    "\n",
    "    def get_session(self):\n",
    "        \"\"\"Get the current SparkSession or create a new one\"\"\"\n",
    "        return self.build_session()\n",
    "\n",
    "    def stop_session(self):\n",
    "        \"\"\"Stop the current Spark session if it exists\"\"\"\n",
    "        if self._session is not None:\n",
    "            self._session.stop()\n",
    "            self._session = None\n",
    "            logger.info(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bc065afbc5a0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:50.910680Z",
     "start_time": "2025-05-07T14:31:48.668937Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Building Spark session with app name: Data Analysis Notebook, master: local[*]\n",
      "25/05/08 16:42:09 WARN Utils: Your hostname, MacBook-Pro-khxng-Wichayada.local resolves to a loopback address: 127.0.0.1; using 10.203.69.21 instead (on interface en0)\n",
      "25/05/08 16:42:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/namin/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/namin/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-59cd2946-0e31-4bc8-aba2-9c003c503197;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/namin/Library/Caches/pypoetry/virtualenvs/traffy-troffi-sQpa58eM-py3.12/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.postgresql#postgresql;42.5.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.426 in central\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (1444ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (388ms)\n",
      ":: resolution report :: resolve 1904ms :: artifacts dl 1838ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.426 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.426] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   2   |   2   |   1   ||   5   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-59cd2946-0e31-4bc8-aba2-9c003c503197\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 3 already retrieved (1234kB/6ms)\n",
      "25/05/08 16:42:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SimpleSparkSession(\n",
    "    app_name=\"Data Analysis Notebook\",\n",
    "    packages=[\n",
    "        \"org.postgresql:postgresql:42.5.4\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.426\"\n",
    "    ],\n",
    "    s3_access_key=os.getenv(\"S3_ACCESS_KEY\"),\n",
    "    s3_secret_key=os.getenv(\"S3_SECRET_KEY\"),\n",
    "    s3_endpoint=os.getenv(\"S3_ENDPOINT\"),\n",
    "    s3_region=\"garage\",\n",
    "    s3_path_style_access=True,\n",
    "    postgres_config={\n",
    "        \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"currentSchema\": \"public\"\n",
    "    },\n",
    "    enable_hive_support=False,\n",
    "    s3_bucket_name=\"traffy-troffi\"\n",
    ").get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3831892c8309fa70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:31:54.313149Z",
     "start_time": "2025-05-07T14:31:53.674801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticket_id: string (nullable = true)\n",
      " |-- complaint: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- image: string (nullable = true)\n",
      " |-- image_after: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- subdistrict: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories_idx: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 16:42:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4363751b6abf68e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:04.742315Z",
     "start_time": "2025-05-07T14:32:04.736952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a376a72faa63c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:12.155924Z",
     "start_time": "2025-05-07T14:32:10.417772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+--------------------+\n",
      "|  ticket_id|           complaint|           timestamp|               image|         image_after|latitude|longitude|district|subdistrict|          categories|      categories_idx|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+--------------------+\n",
      "|2024-8AKXPM|มีบ้านปล่อยน้ำจาก...|2024-11-06 08:35:...|https://storage.g...|https://storage.g...|13.75058|100.53849| ราชเทวี|ถนนเพชรบุรี|               [ถนน]|               [9.0]|\n",
      "|2024-KPTM7X|พบคนไร้บ้านนอนที่...|2024-06-14 20:21:...|https://storage.g...|https://storage.g...|13.76059|100.53852| ราชเทวี|   ถนนพญาไท|[ทางเท้า, คนจรจัด...|    [10.0, 3.0, 9.0]|\n",
      "|2024-MP49WU|14/5/67 ตามที่เคย...|2024-08-03 14:19:...|https://storage.g...|https://storage.g...|13.76449|100.53927| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-9L2LUN|14/5/67 ตามที่เคย...|2024-07-23 14:25:...|https://storage.g...|https://storage.g...|13.76455|100.53929| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-LM4VE9|14/5/67 ตามที่เคย...|2024-07-23 11:11:...|https://storage.g...|https://storage.g...|13.76459| 100.5393| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-DFWVXZ|บริเวณถนนเพชรบุรี...|2024-06-17 05:53:...|https://storage.g...|https://storage.g...|13.75169|100.53562| ราชเทวี|ถนนเพชรบุรี|      [ถนน, กีดขวาง]|          [9.0, 2.0]|\n",
      "|2024-674LPF|14/5/67 ตามที่เคย...|2024-07-23 14:24:...|https://storage.g...|https://storage.g...|13.76438|100.53924| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-4W4F4L|ถนนเพชรบุรีบริเวณ...|2024-06-20 04:22:...|https://storage.g...|https://storage.g...|13.75179|100.53567| ราชเทวี|ถนนเพชรบุรี|        [จราจร, ถนน]|          [7.0, 9.0]|\n",
      "|2024-36HHKK|ถนนราชปรารภ ฝั่ง ...|2024-06-17 06:04:...|https://storage.g...|https://storage.g...| 13.7534|100.54176| ราชเทวี|   ถนนพญาไท|[กีดขวาง, ถนน, จร...|     [2.0, 9.0, 7.0]|\n",
      "|2024-KCB8B2|บนสะพานลอย ถนนเพช...|2024-06-20 11:54:...|https://storage.g...|https://storage.g...|13.75201|100.53523| ราชเทวี|ถนนเพชรบุรี|[คนจรจัด, ความปลอ...|[3.0, 5.0, 9.0, 2.0]|\n",
      "|2024-MLWK99|ขายของกลางถนน ไม่...|2024-06-25 13:06:...|https://storage.g...|https://storage.g...|13.75417|100.53777| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-4WRFM6|มีหลุมตรงร่องฝา ก...|2024-06-28 02:44:...|https://storage.g...|https://storage.g...|13.76209|100.52601| ราชเทวี|  ทุ่งพญาไท|  [ความปลอดภัย, ถนน]|          [5.0, 9.0]|\n",
      "|2024-HT7NX7|เห็นเดินอยู่แต่แถ...|2025-01-14 15:24:...|https://storage.g...|https://storage.g...|13.76321|100.54253| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "|2024-NFELAT|ถนนเพชรบุรี บริเว...|2024-06-21 06:50:...|https://storage.g...|https://storage.g...|13.75175|100.53545| ราชเทวี|ถนนเพชรบุรี|      [ถนน, กีดขวาง]|          [9.0, 2.0]|\n",
      "|2024-GNQ7AW|ถนนเป็นหลมย่อลึกเ...|2024-12-06 08:12:...|https://storage.g...|https://storage.g...|13.75895|100.54241| ราชเทวี|   ถนนพญาไท|  [ความปลอดภัย, ถนน]|          [5.0, 9.0]|\n",
      "|2024-343MWB|ถนนเพชรบุรี ปากซอ...|2024-06-21 06:50:...|https://storage.g...|https://storage.g...|13.75183|100.53553| ราชเทวี|ถนนเพชรบุรี|      [กีดขวาง, ถนน]|          [2.0, 9.0]|\n",
      "|2024-GGPQDM|ซากรถบนถนน จตุรทิ...|2024-11-02 02:48:...|https://storage.g...|https://storage.g...|13.75479|100.55485| ราชเทวี|   มักกะสัน|               [ถนน]|               [9.0]|\n",
      "|2024-M9MLFV|สัญญาณไฟเขียวแดงส...|2024-09-12 08:04:...|https://storage.g...|https://storage.g...|13.75938|100.54106| ราชเทวี|   ถนนพญาไท|[ต้นไม้, ถนน, จราจร]|     [8.0, 9.0, 7.0]|\n",
      "|2024-A6XNXW|ชาวต่างชาติ โพสคล...|2024-06-26 00:42:...|https://storage.g...|https://storage.g...|13.76914|100.52898| ราชเทวี|  ทุ่งพญาไท|        [สะพาน, ถนน]|         [17.0, 9.0]|\n",
      "|2024-FU2G9Q|แจ้งหน่วยงานที่เก...|2024-07-04 03:00:...|https://storage.g...|https://storage.g...|13.75956|100.54099| ราชเทวี|   ถนนพญาไท|               [ถนน]|               [9.0]|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------+---------+--------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).filter(\n",
    "    (F.col(\"district\") == 'ราชเทวี') & (F.array_contains('categories', 'ถนน'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06acc14a916a03f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:17.965653Z",
     "start_time": "2025-05-07T14:32:17.684488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(table='traffy_fondue',\n",
    "                url=\"jdbc:postgresql://localhost:5432/traffy-troffi\",\n",
    "                properties={\"user\": \"postgres\", \"password\": \"troffi\",\n",
    "                            \"driver\": \"org.postgresql.Driver\",\n",
    "                            \"currentSchema\": \"public\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e934b33b9f6a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffy-troffi-sQpa58eM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
